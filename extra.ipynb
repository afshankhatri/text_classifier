{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6521bf-47c4-42b2-bd46-7effd4eaac98",
   "metadata": {},
   "source": [
    "# Regular Expression (re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4fda11-5cd5-4caf-8044-d1a32ef111cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33c45fe-b8a6-4c5c-9fdc-f64df16bde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"hero\":\"salman Khan\",\n",
    "    \"movie\" : \"tere naam\",\n",
    "    \"description\" : \"this movie is an UA catogary movie. it is an emotinal picture where it tells us to stroy of a heart broken man .in such a way that your tears wont find a way to hide behing you eyes.\",\n",
    "    \"ratings\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206bf6f7-b7c8-4159-8701-b646997746e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"ratings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e18493-0fce-46e6-8b7f-4a7416124d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://twitter.com/elonmusk,',\n",
       " 'https://twitter.com/teslarati',\n",
       " 'https://twitter.com/dummy_tesla',\n",
       " 'https://twitter.com/dummy_2_tesla']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information\n",
    "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers\n",
    "for tesla related news,\n",
    "https://twitter.com/teslarati\n",
    "https://twitter.com/dummy_tesla\n",
    "https://twitter.com/dummy_2_tesla\n",
    "'''\n",
    "\n",
    "\n",
    "# \\S+ => One or more non-whitespace characters\tMatches only words (no empty results)\n",
    "# \\S* => Zero or more non-whitespace characters...Matches words and empty strings\n",
    "# NOTE : small(\\s) is for space and the caps(\\S) if for string. \n",
    "\n",
    "pattern = r'https://twitter\\.com/dummy\\S*\\S+' # this will print the links which has dummy releted terms in it (in the URL's)\n",
    "pattern1 = r'https://twitter\\.com\\S+' # this will print all the twitter relted links \n",
    "\n",
    "re.findall(pattern1,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85cf3359-06bf-4faf-9428-9eec222ad3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+91-91525-49454',\n",
       " '(+91)-98674-24218',\n",
       " '+91-77380-71719',\n",
       " '(+91)-93227-08511',\n",
       " '919167287827',\n",
       " '(+971)-55678-9954']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = '+91-91525-49454 , (+91)-98674-24218 , +91-77380-71719 , (+91)-93227-08511,919167287827,(+971)-55678-9954' \n",
    "email = 'afshan@gmail.com'\n",
    "orders = 'order#9547856 , order: #44511225412, order # 7485628525652,Order number 84141451'\n",
    "\n",
    "pattern0 = r'\\d'  # this will print single number\n",
    "pattern = r'\\d{12}'  # this will print if the number is in sequential order\n",
    "pattern1 = r'\\(\\d{3}\\)'  # this is how we take the numbers which are in bracket\n",
    "pattern2 = r'\\S*'\n",
    "pattern3 = r'[a-z0-9A-Z_]*@[a-z0-9A-Z.]*[a-z]' # for gmails .... [a-z0-9A-Z_] this will take small/caps a-z ,0-9 numbs and \"_\" char... '*' will attach the strings & numbers\n",
    "pattern4 = r'order[^\\d]*(\\d*)' \n",
    "pattern4b= r'order[^\\d]*(\\d*)|Order[^\\d]*(\\d*)' #alternate of pattern4\n",
    "\n",
    "pattern5 = r'\\(?\\+?\\d{1,4}\\)?[-\\s]?\\d{5}[-\\s]?\\d{3,5}'  # \\d{3,5} -> this means that the number can be of 3 to 5 digits\n",
    "\n",
    "\n",
    "# re.findall(pattern,numbers)\n",
    "# re.findall(pattern4,orders,re.IGNORECASE) # this will work irrespective of the case size of the alphabets\n",
    "re.findall(pattern5,numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907faedd-0e78-4cb2-92f1-e8df3f1fcc92",
   "metadata": {},
   "source": [
    "# Spacy & Nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244de924-f1d2-453e-bc8c-e4d732c58c82",
   "metadata": {},
   "source": [
    "**spacy is like a mobile phone which capture the image of a person then analyses it , makes it better ,auto-focus and with enhanced features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe42ef-44eb-40bb-97fb-2a3a4f3c1a1f",
   "metadata": {},
   "source": [
    "**where as ,NLTK is like a professional camera which gives us full liberty to click the picture according to our requirement and everything has to be done by us ,if we do it properly it give best ourput else it wont give good o/p**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3eccb8-57e0-444c-9204-8e1ca4f75676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe014275-69f2-4b52-90db-4d3dfba4ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # this is to stor ethe en land lib releted content in the nlp variabble\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6794dfba-e3c4-4f81-9deb-e3d881765fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = nlp(\"Dr. starange here . I can do many things including protecting the world , reversing the time etc. yeah that is all from my side\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37cbb26-e5c2-43d9-96e3-f2ba761b8fcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m.\u001b[49m\u001b[43msents\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#  (segmentation)  this will directly brak the statement into seneteces \u001b[39;49;00m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (tokenization) this will break the above created sentences into single words (word can also be meaninglesss)\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# print(word)  this will just do tokenization \u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\spacy\\tokens\\doc.pyx:926\u001b[39m, in \u001b[36msents\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "for sentence in statement.sents: #  (segmentation)  this will directly brak the statement into seneteces \n",
    "    print(sentence)\n",
    "    for word in sentence: # (tokenization) this will break the above created sentences into single words (word can also be meaninglesss)\n",
    "        # print(word)  this will just do tokenization \n",
    "        print(word.lemma_) # (lemmatization) this will bring the word to its root words \n",
    "\n",
    "        # lemmatization and stemming is somewhat similar ...\n",
    "        # Stemming just chops off word endings without caring about meaning...\n",
    "        # while lemmatization finds the correct meaningful base form of the word by understanding its context and grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f374b097-c902-4b44-a497-f560fae0f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1ffa7-ec6a-488a-88c2-ec58d55f822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()  # when we download nltk ... all the packefes are not installed we need to download according to our need .when we give this command we will get a pop up window which will ask that which part to install .if we need all then click all and download all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eac4f4b0-67c6-4a12-9aef-9628ba5d9c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'starange',\n",
       " 'here',\n",
       " '.',\n",
       " 'I',\n",
       " 'can',\n",
       " 'do',\n",
       " 'many',\n",
       " 'things',\n",
       " 'including',\n",
       " 'protecting',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'reversing',\n",
       " 'the',\n",
       " 'time',\n",
       " 'etc',\n",
       " '.',\n",
       " 'yeah',\n",
       " 'that',\n",
       " 'is',\n",
       " 'all',\n",
       " 'from',\n",
       " 'my',\n",
       " 'side']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"Dr. starange here . I can do many things including protecting the world , reversing the time etc. yeah that is all from my side\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd84e5f0-c39d-4132-a67d-712aa21774f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr. starange here .',\n",
       " 'I can do many things including protecting the world , reversing the time etc.',\n",
       " 'yeah that is all from my side']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"Dr. starange here . I can do many things including protecting the world , reversing the time etc. yeah that is all from my side\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648bed1-1349-45f0-8cd1-70a6ee5a9312",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d31f4e5-e6d4-42b5-8a81-8acb5f6d79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = statement[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7078261-bcb0-4d44-9d93-9c0f97775e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19b1829e-bd2f-4f3f-856b-38946b516755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.is_alpha # this will tell us weather it is alphabet or no ... there are various other attri ute within it which we can use to identify things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27e86e9f-52f2-4544-8e45-518a5ba958a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token) # here's the list of all the attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc439610-d029-4e96-858e-242fabf1eb9d",
   "metadata": {},
   "source": [
    "**iterating over the statement and mathing the attributes and understanding it in detail** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6976925-995c-480a-a59a-12605eb3d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. index 0 is_num: False is_alpha: False\n",
      "starange index 1 is_num: False is_alpha: True\n",
      "here index 2 is_num: False is_alpha: True\n",
      ". index 3 is_num: False is_alpha: False\n",
      "I index 4 is_num: False is_alpha: True\n",
      "can index 5 is_num: False is_alpha: True\n",
      "do index 6 is_num: False is_alpha: True\n",
      "many index 7 is_num: False is_alpha: True\n",
      "things index 8 is_num: False is_alpha: True\n",
      "including index 9 is_num: False is_alpha: True\n",
      "protecting index 10 is_num: False is_alpha: True\n",
      "the index 11 is_num: False is_alpha: True\n",
      "world index 12 is_num: False is_alpha: True\n",
      ", index 13 is_num: False is_alpha: False\n",
      "reversing index 14 is_num: False is_alpha: True\n",
      "the index 15 is_num: False is_alpha: True\n",
      "time index 16 is_num: False is_alpha: True\n",
      "etc index 17 is_num: False is_alpha: True\n",
      ". index 18 is_num: False is_alpha: False\n",
      "yeah index 19 is_num: False is_alpha: True\n",
      "that index 20 is_num: False is_alpha: True\n",
      "is index 21 is_num: False is_alpha: True\n",
      "all index 22 is_num: False is_alpha: True\n",
      "from index 23 is_num: False is_alpha: True\n",
      "my index 24 is_num: False is_alpha: True\n",
      "side index 25 is_num: False is_alpha: True\n"
     ]
    }
   ],
   "source": [
    "for token in statement:\n",
    "    print(token,\"index\",token.i,\n",
    "          \"is_num:\",token.like_num,\n",
    "          \"is_alpha:\",token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "083e8cc4-5679-4b2f-a0cf-a894c9ea7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5dbb3-63f5-4d7e-88c0-e3b2483a3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "data = {\n",
    "    'Name': ['Virat', 'Maria', 'Serena', 'Joe'],\n",
    "    'birth day': ['5 June, 1882', '12 April, 2001', '24 June, 1998', '1 May, 1997'],\n",
    "    'email': [\n",
    "        'virat@kohli.com',\n",
    "        'maria@sharapova.com',\n",
    "        'serena@williams.com',\n",
    "        'joe@root.com'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10d33800-09eb-4f47-b5d6-4cd5b3c42b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StudData\",\"w\") as f:\n",
    "    f.write(df.to_string(index=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dea863a-8616-4968-96a7-90f65bb30725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Name      birth day               email\\n',\n",
       " ' Virat   5 June, 1882     virat@kohli.com\\n',\n",
       " ' Maria 12 April, 2001 maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998 serena@williams.com\\n',\n",
       " '   Joe    1 May, 1997        joe@root.com']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (\"StudData\",\"r\") as f :\n",
    "    prin = f.readlines()  # if we write f.read() it will read word-by-word ...... but f.readlines() will read whole line together\n",
    "prin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0f3f43b-e70c-4bb6-baf0-e84a69fa8aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Name      birth day               email\\n  Virat   5 June, 1882     virat@kohli.com\\n  Maria 12 April, 2001 maria@sharapova.com\\n Serena  24 June, 1998 serena@williams.com\\n    Joe    1 May, 1997        joe@root.com'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prin = ' '.join(prin)\n",
    "prin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "552b4075-a2c0-4fa8-a840-fb84e9fbf80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emai ID => virat@kohli.com\n",
      "emai ID => maria@sharapova.com\n",
      "emai ID => serena@williams.com\n",
      "emai ID => joe@root.com\n"
     ]
    }
   ],
   "source": [
    "store =nlp(prin)  # this is very important line if we want any text to get converted in to nlp terms\n",
    "for word in store:\n",
    "    if(word.like_email):\n",
    "        print(\"emai ID =>\",word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e3797-7309-4453-b0f7-e175c0c2f48a",
   "metadata": {},
   "source": [
    "**exerceise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d2d451e-1c99-4ad5-896d-eeb76f9b95f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.data.gov/\n",
      "http://www.science\n",
      "http://data.gov.uk/.\n",
      "http://www3.norc.org/gss+website/\n",
      "http://www.europeansocialsurvey.org/.\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "text = '''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/,\n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "# method1\n",
    "# text = ''.join(text)\n",
    "# text\n",
    "# pattern = r\"https?://[a-z0-9A-Z./+]*\"\n",
    "# pattern = r'https?://[^\\s)\"]+'  # \"^\" this sign in a bracket indicated that we dont have to accept the foll. in the string .... here it says dont acc anything that is \\s(white space),\"(single side of doubl quote), ) closing side of paranthesis \n",
    "# re.findall(pattern,text)\n",
    "\n",
    "# method2\n",
    "text = nlp(text)\n",
    "for link in text:\n",
    "    if (link.like_url):\n",
    "        print(link)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "321cbf40-c02a-490e-9d96-7f010444c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "# 2 \n",
    "transaction = \"Tony gave two $ to peter ,Bruce gave 500 € to steve\"\n",
    "trans = nlp(transaction)\n",
    "for money in trans:\n",
    "    if(money.like_num and trans[money.i+1].is_currency):\n",
    "        print(money,trans[money.i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77f44f-7a16-4b26-82ab-6979635c50bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
